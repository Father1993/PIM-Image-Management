import aiohttp
import asyncio
import os
import base64
from urllib.parse import urlparse
import logging
from datetime import datetime
from supabase import create_client
from dotenv import load_dotenv

# ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ð»Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(message)s")
logger = logging.getLogger()

try:
    from dotenv import load_dotenv

    load_dotenv()
except ImportError:
    pass

# ÐšÐ¾Ð½Ñ„Ð¸Ð³
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")
IMGPROXY_URL = os.getenv("IMGPROXY_URL")
BUCKET_NAME = "optimized"  # Ð±Ð°ÐºÐµÑ‚ Ð´Ð»Ñ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ñ… ÐºÐ°Ñ€Ñ‚Ð¸Ð½Ð¾Ðº
BATCH_SIZE = 100  # Ð¾Ð´Ð½Ð¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾ Ð¾Ð±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÐµÐ¼ Ð¿Ð¾ 100 Ð¿Ñ€Ð¾Ð´ÑƒÐºÑ‚Ð¾Ð²

supabase = create_client(SUPABASE_URL, SUPABASE_KEY)


async def ensure_bucket_exists():
    """Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ð¼ bucket, ÐµÑÐ»Ð¸ Ð¾Ð½ ÐµÑ‰Ñ‘ Ð½Ðµ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÐµÑ‚"""
    async with aiohttp.ClientSession() as session:
        headers = {
            "apikey": SUPABASE_KEY,
            "Authorization": f"Bearer {SUPABASE_KEY}",
            "Content-Type": "application/json",
        }
        async with session.get(f"{SUPABASE_URL}/storage/v1/bucket", headers=headers) as resp:
            if resp.status == 200:
                buckets = await resp.json()
                if any(b["name"] == BUCKET_NAME for b in buckets):
                    logger.info(f"âœ… Bucket '{BUCKET_NAME}' ÑƒÐ¶Ðµ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÐµÑ‚")
                    return

        payload = {"name": BUCKET_NAME, "public": True}
        async with session.post(f"{SUPABASE_URL}/storage/v1/bucket", headers=headers, json=payload) as resp:
            if resp.status in (200, 201):
                logger.info(f"ðŸ“‚ Ð¡Ð¾Ð·Ð´Ð°Ð½ Ð½Ð¾Ð²Ñ‹Ð¹ bucket '{BUCKET_NAME}'")
            else:
                text = await resp.text()
                logger.error(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ bucket: {resp.status} {text}")


async def optimize_image(session, image_url: str) -> bytes | None:
    """ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ Ñ‡ÐµÑ€ÐµÐ· imgproxy Ð´Ð¾ 750x1000 Ñ Ð±ÐµÐ»Ñ‹Ð¼ Ñ„Ð¾Ð½Ð¾Ð¼"""
    try:
        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð¸ÑÑ…Ð¾Ð´Ð½Ñ‹Ð¹ URL, Ð¿Ñ€Ð¸ 404 Ð¿Ñ€Ð¾Ð±ÑƒÐµÐ¼ Ñ Ð½Ð¸Ð¶Ð½Ð¸Ð¼ Ñ€ÐµÐ³Ð¸ÑÑ‚Ñ€Ð¾Ð¼ Ñ€Ð°ÑÑˆÐ¸Ñ€ÐµÐ½Ð¸Ñ
        async with session.head(image_url) as check:
            if check.status != 200:
                root, ext = os.path.splitext(image_url)
                if ext and ext.lower() != ext:
                    alt_url = root + ext.lower()
                    async with session.head(alt_url) as check2:
                        if check2.status == 200:
                            image_url = alt_url
                        else:
                            logger.warning(f"Ð˜Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð¾: {image_url}")
                            return None
                else:
                    logger.warning(f"Ð˜Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð¾: {image_url}")
                    return None

        # ÐšÐ¾Ð´Ð¸Ñ€ÑƒÐµÐ¼ URL Ð´Ð»Ñ imgproxy
        b64_url = base64.urlsafe_b64encode(image_url.encode()).decode().rstrip("=")
        
        # resize:fit - ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð¿Ñ€Ð¾Ð¿Ð¾Ñ€Ñ†Ð¸Ð¹, extend:1:ce - Ð±ÐµÐ»Ñ‹Ð¹ Ñ„Ð¾Ð½ Ð¿Ð¾ Ñ†ÐµÐ½Ñ‚Ñ€Ñƒ
        imgproxy_url = f"{IMGPROXY_URL}/unsafe/resize:fit:750:1000/extend:1:ce/background:255:255:255/quality:85/{b64_url}.jpg"

        
        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ðµ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ
        async with session.get(imgproxy_url) as resp:
            if resp.status == 200:
                return await resp.read()
            logger.warning(f"ÐžÑˆÐ¸Ð±ÐºÐ° imgproxy {resp.status} Ð´Ð»Ñ {image_url}")
            return None

    except Exception as e:
        logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸ {image_url}: {e}")
    return None


async def upload_to_supabase(image_name: str, data: bytes) -> str | None:
    """Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ð³Ð¾ Ñ„Ð°Ð¹Ð»Ð° Ð² Supabase Storage Ñ Ð¿Ð°Ð¿ÐºÐ°Ð¼Ð¸ Ð¿Ð¾ Ð´Ð°Ñ‚Ð°Ð¼"""
    try:
        today = datetime.now()
        path = f"{today.year}/{today.month:02d}/{today.day:02d}/{image_name}.JPG"
        options = {"content-type": "image/jpeg", "upsert": "true"}
        try:
            supabase.storage.from_(BUCKET_NAME).upload(path, data, options)
        except Exception as e:
            msg = str(e)    
            if "Duplicate" in msg or "already exists" in msg or "409" in msg:
                # Ð¤Ð°Ð¹Ð» ÑƒÐ¶Ðµ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÐµÑ‚ â€” ÑÑ‡Ð¸Ñ‚Ð°ÐµÐ¼ ÐºÐ°Ðº ÑƒÑÐ¿ÐµÑ…
                pass
            else:
                raise

        public_url = supabase.storage.from_(BUCKET_NAME).get_public_url(path)
        if isinstance(public_url, str) and public_url.endswith("?"):
            public_url = public_url[:-1]
        return public_url
    except Exception as e:
        logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ {image_name}: {e}")
        return None


async def process_image(session, product: dict, index: int, total: int):
    """ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ + Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¾Ð´Ð½Ð¾Ð³Ð¾ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ Ð¿Ñ€Ð¾Ð´ÑƒÐºÑ‚Ð°"""
    product_id = product["id"]
    product_name = product.get("product_name", "")
    image_url = product.get("image_url")
    
    if not image_url or not str(image_url).strip():
        logger.info(f"[{index}/{total}] ÐŸÑ€Ð¾Ð¿ÑƒÑÐº {product_name}: Ð¿ÑƒÑÑ‚Ð¾Ð¹ image_url")
        return False
    
    # Ð¤Ð¾Ñ€Ð¼Ð¸Ñ€ÑƒÐµÐ¼ Ð¸Ð¼Ñ Ñ„Ð°Ð¹Ð»Ð° Ð¸Ð· URL Ð¸Ð»Ð¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ product_id
    url_path = urlparse(image_url).path
    file_from_url = os.path.basename(url_path)
    image_name = os.path.splitext(file_from_url)[0] or f"product_{product_id}"

    logger.info(f"[{index}/{total}] ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° {product_name or image_name} ({image_url})")

    data = await optimize_image(session, image_url)
    if not data:
        return False

    new_url = await upload_to_supabase(image_name, data)
    if not new_url:
        return False

    supabase.table("products").update(
        {
            "is_optimized": True,
            "optimized_url": new_url,
            "updated_at_image_optimized": datetime.now().isoformat(),
        }
    ).eq("id", product_id).execute()

    logger.info(f"[{index}/{total}] âœ… Ð£ÑÐ¿ÐµÑˆÐ½Ð¾: {product_name or image_name} â†’ {new_url}")
    return True


async def main(limit: int | None = None):
    """ÐžÑÐ½Ð¾Ð²Ð½Ð¾Ð¹ Ñ†Ð¸ÐºÐ»"""
    logger.info("ðŸš€ Ð—Ð°Ð¿ÑƒÑÐº Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸ ÐºÐ°Ñ€Ñ‚Ð¸Ð½Ð¾Ðº Ð¸Ð· Supabase")

    await ensure_bucket_exists()

    # Ð—Ð°Ð¿Ñ€Ð¾Ñ Ð¿Ñ€Ð¾Ð´ÑƒÐºÑ‚Ð¾Ð² Ñ Ð½ÐµÐ¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¼Ð¸ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸ÑÐ¼Ð¸
    query = (
        supabase
        .table("products")
        .select("id, product_name, image_url, is_optimized")
        .or_("is_optimized.is.null,is_optimized.eq.false")
        .not_.is_("image_url", "null")
        .neq("image_url", "")
    )
    if limit:
        query = query.limit(limit)

    products = query.execute().data or []
    logger.info(f"ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ {len(products)} Ð¿Ñ€Ð¾Ð´ÑƒÐºÑ‚Ð¾Ð² Ð´Ð»Ñ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸")

    if not products:
        logger.info("ÐÐµÑ‚ Ð¿Ñ€Ð¾Ð´ÑƒÐºÑ‚Ð¾Ð² Ð´Ð»Ñ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸")
        return

    async with aiohttp.ClientSession() as session:
        total = len(products)
        processed = 0
        success = 0

        for i in range(0, total, BATCH_SIZE):
            batch = products[i : i + BATCH_SIZE]
            # Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ð¼ Ð·Ð°Ð´Ð°Ñ‡Ð¸ Ñ Ð½ÑƒÐ¼ÐµÑ€Ð°Ñ†Ð¸ÐµÐ¹ Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ Ð¿Ñ€Ð¾Ð´ÑƒÐºÑ‚Ð°
            tasks = [
                process_image(session, product, i + idx + 1, total)
                for idx, product in enumerate(batch)
            ]
            results = await asyncio.gather(*tasks)
            processed += len(batch)
            success += sum(1 for r in results if r)
            logger.info(f"ðŸ“Š ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: {success} ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ / {processed} Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð¾ / {total} Ð²ÑÐµÐ³Ð¾")

    logger.info(f"ðŸŽ‰ Ð“Ð¾Ñ‚Ð¾Ð²Ð¾: {success}/{len(products)} Ð¿Ñ€Ð¾Ð´ÑƒÐºÑ‚Ð¾Ð² Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¾")


if __name__ == "__main__":
    import sys

    limit = int(sys.argv[1]) if len(sys.argv) > 1 and sys.argv[1].isdigit() else None
    asyncio.run(main(limit))
